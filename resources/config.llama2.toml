prompt_template = "templates/clippy_prompt.txt"

[model]
# Download from https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGML
# NOTE: Path is relative from this file.
path = "models/LLaMa2/llama-2-7b-chat.ggmlv3.q4_1.bin"
model_type = "Llama"
prefer_mmap = false
# The top K words by score are kept during sampling.
top_k = 40
# The cumulative probability after which no more words are kept for sampling.
top_p = 0.95
# The penalty for repeating tokens. Higher values make the generation less
# likely to get into a loop, but may harm results when repetitive outputs
# are desired.
repeat_penalty = 1.30
# Temperature (randomness) used for sampling. A higher number is more random.
temperature = 0.2
# The number of tokens to consider for the repetition penalty.
repetition_penalty_last_n = 512